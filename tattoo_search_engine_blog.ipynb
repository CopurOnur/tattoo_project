{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an AI-Powered Tattoo Search Engine: A Deep Dive\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Finding the perfect tattoo design can be challenging. What if you could upload an image and instantly find visually similar tattoo designs from across the web? This is exactly what I built - an AI-powered tattoo search engine that combines state-of-the-art computer vision models with intelligent web scraping.\n",
    "\n",
    "**Live Demo:**\n",
    "- Frontend: https://tattoo-search-frontend.vercel.app\n",
    "- Backend API: https://onurcopur-tattoo-search-engine.hf.space\n",
    "\n",
    "In this blog post, I'll take you through the technical architecture, design decisions, and implementation details of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "### What Does It Do?\n",
    "\n",
    "The Tattoo Search Engine allows users to:\n",
    "1. **Upload a tattoo image** (or any image for inspiration)\n",
    "2. **Get AI-generated captions** describing the tattoo style\n",
    "3. **Search multiple platforms** (Pinterest, Reddit, Instagram) for similar designs\n",
    "4. **Rank results by visual similarity** using advanced embedding models\n",
    "5. **Analyze patch-level attention** to understand which parts of images are most similar\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Multi-Model Support**: Choose from CLIP, DINOv2, DINOv3, or SigLIP embedding models\n",
    "- **Visual Similarity Search**: Not just keyword matching - actual visual understanding\n",
    "- **Patch-Level Analysis**: See exactly which regions of images correspond\n",
    "- **Multi-Platform Scraping**: Aggregates results from multiple sources\n",
    "- **Production-Ready**: Deployed with Docker, optimized for GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "The project is organized as a **monorepo** with two main components:\n",
    "\n",
    "```\n",
    "tattoo_project/\n",
    "├── tattoo_search_engine/          # Python FastAPI Backend\n",
    "│   ├── main.py                    # Core orchestration\n",
    "│   ├── embeddings.py              # Model abstraction layer\n",
    "│   ├── patch_attention.py         # Visual analysis\n",
    "│   ├── search_engines/            # Multi-platform search\n",
    "│   │   ├── manager.py\n",
    "│   │   ├── pinterest.py\n",
    "│   │   ├── reddit.py\n",
    "│   │   └── instagram.py\n",
    "│   └── utils/                     # Caching & validation\n",
    "│       ├── cache.py\n",
    "│       └── url_validator.py\n",
    "└── tattoo_search_engine_frontend/ # Next.js TypeScript Frontend\n",
    "    ├── pages/\n",
    "    ├── components/\n",
    "    └── types/\n",
    "```\n",
    "\n",
    "### Technology Stack\n",
    "\n",
    "#### Backend\n",
    "- **Framework**: FastAPI (Python)\n",
    "- **ML Models**: PyTorch, Transformers, OpenCLIP, TIMM\n",
    "- **Vision Models**: CLIP, DINOv2, DINOv3, SigLIP\n",
    "- **Captioning**: GLM-4.5V via HuggingFace InferenceClient\n",
    "- **Web Scraping**: DuckDuckGo Search, Requests, LXML\n",
    "- **Deployment**: Docker on HuggingFace Spaces (GPU-enabled)\n",
    "\n",
    "#### Frontend\n",
    "- **Framework**: Next.js 14 (Pages Router) with TypeScript\n",
    "- **Styling**: Tailwind CSS\n",
    "- **Deployment**: Vercel\n",
    "- **State Management**: React Hooks\n",
    "\n",
    "#### Infrastructure\n",
    "- **Backend Host**: HuggingFace Spaces (T4 GPU recommended)\n",
    "- **Frontend Host**: Vercel\n",
    "- **Port**: 7860 (HF Spaces default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive: Backend Architecture\n",
    "\n",
    "### The Core Pipeline\n",
    "\n",
    "Let's trace what happens when you upload an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified flow from main.py\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_tattoos(\n",
    "    file: UploadFile,\n",
    "    embedding_model: str = \"clip\",\n",
    "    include_patch_attention: bool = False\n",
    "):\n",
    "    # 1. Load the image\n",
    "    query_image = Image.open(io.BytesIO(image_data))\n",
    "    \n",
    "    # 2. Generate caption using GLM-4.5V\n",
    "    caption = engine.generate_caption(query_image)\n",
    "    # Example: \"geometric mandala tattoo with intricate patterns\"\n",
    "    \n",
    "    # 3. Search multiple platforms\n",
    "    candidate_urls = engine.search_images(caption, max_results=100)\n",
    "    \n",
    "    # 4. Compute visual similarity\n",
    "    results = engine.compute_similarity(\n",
    "        query_image, candidate_urls, include_patch_attention\n",
    "    )\n",
    "    \n",
    "    return {\"caption\": caption, \"results\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 1: Image Captioning\n",
    "\n",
    "#### Why Captioning?\n",
    "\n",
    "While we could do pure visual search, combining it with text-based search dramatically improves results:\n",
    "- **Contextual understanding**: \"tribal sleeve tattoo\" vs \"floral ankle tattoo\"\n",
    "- **Style recognition**: \"watercolor\", \"realistic\", \"minimalist\"\n",
    "- **Better initial results**: Text search casts a wider net\n",
    "\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(self, image: Image.Image) -> str:\n",
    "    # Convert image to base64\n",
    "    img_buffer = io.BytesIO()\n",
    "    image.save(img_buffer, format=\"JPEG\", quality=95)\n",
    "    image_b64 = base64.b64encode(img_buffer.getvalue()).decode()\n",
    "    image_url = f\"data:image/jpeg;base64,{image_b64}\"\n",
    "    \n",
    "    # Call GLM-4.5V via HuggingFace InferenceClient\n",
    "    completion = self.client.chat.completions.create(\n",
    "        model=\"zai-org/GLM-4.5V\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \n",
    "                 \"text\": \"Generate a one search engine query to find similar tattoos...\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "            ]\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Extract JSON search query\n",
    "    caption = completion.choices[0].message.content\n",
    "    data = json.loads(re.search(r\"\\{.*\\}\", caption).group())\n",
    "    return data[\"search_query\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Design Choice**: Using GLM-4.5V via Novita provider gives us:\n",
    "- High-quality vision-language understanding\n",
    "- Structured JSON output for reliable parsing\n",
    "- Graceful fallback to \"tattoo artwork\" if captioning fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Multi-Platform Search Engine\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "The search system uses a **tiered fallback strategy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From search_engines/manager.py\n",
    "\n",
    "class SearchEngineManager:\n",
    "    def search_with_fallback(\n",
    "        self,\n",
    "        query: str,\n",
    "        max_results: int = 50,\n",
    "        min_results_threshold: int = 10\n",
    "    ):\n",
    "        # Tier 1: Primary platforms (Pinterest, Reddit)\n",
    "        results = self._search_platforms(\n",
    "            [SearchPlatform.PINTEREST, SearchPlatform.REDDIT],\n",
    "            query, max_results\n",
    "        )\n",
    "        \n",
    "        if len(results) >= min_results_threshold:\n",
    "            return results\n",
    "        \n",
    "        # Tier 2: Add Instagram\n",
    "        additional = self._search_platforms(\n",
    "            [SearchPlatform.INSTAGRAM], query, max_results\n",
    "        )\n",
    "        results.extend(additional)\n",
    "        \n",
    "        if len(results) >= min_results_threshold:\n",
    "            return results\n",
    "        \n",
    "        # Tier 3: Simplify query and retry\n",
    "        simplified_query = self._simplify_query(query)\n",
    "        fallback_results = self._search_all_platforms(simplified_query)\n",
    "        \n",
    "        return results + fallback_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Platform Implementations\n",
    "\n",
    "Each platform has its own search engine class:\n",
    "\n",
    "**Pinterest Search:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinterestSearchEngine(BaseSearchEngine):\n",
    "    def search(self, query: str, max_results: int) -> List[ImageResult]:\n",
    "        # Use DuckDuckGo with site:pinterest.com\n",
    "        with DDGS() as ddgs:\n",
    "            results = ddgs.images(\n",
    "                f\"{query} site:pinterest.com\",\n",
    "                max_results=max_results\n",
    "            )\n",
    "        \n",
    "        return [ImageResult(\n",
    "            url=result['image'],\n",
    "            source_url=result.get('url', ''),\n",
    "            platform=SearchPlatform.PINTEREST\n",
    "        ) for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Multiple Platforms?**\n",
    "- **Diversity**: Different platforms have different content\n",
    "- **Resilience**: If one platform fails, others continue\n",
    "- **Quality**: More candidates = better final results after visual ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Embedding Models\n",
    "\n",
    "This is where the magic happens. The system supports **5 different embedding models**, each with unique strengths.\n",
    "\n",
    "#### Model Abstraction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From embeddings.py\n",
    "\n",
    "class EmbeddingModel(ABC):\n",
    "    \"\"\"Abstract base class for embedding models.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def encode_image(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"Encode image into global feature vector.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def encode_image_patches(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"Encode image into patch-level features.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def compute_similarity(self, query_features, candidate_features) -> float:\n",
    "        \"\"\"Compute cosine similarity.\"\"\"\n",
    "        return torch.mm(query_features, candidate_features.T).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supported Models\n",
    "\n",
    "| Model | Description | Best For |\n",
    "|-------|-------------|----------|\n",
    "| **CLIP** (OpenAI) | Vision-language model trained on image-text pairs | General purpose, understands style descriptions |\n",
    "| **DINOv2** (Meta) | Self-supervised vision transformer | Fine-grained visual details, artistic styles |\n",
    "| **DINOv2 w/ Registers** | DINOv2 with register tokens for better attention | Improved feature maps, patch analysis |\n",
    "| **DINOv3** (Meta) | Latest DINO with high-quality dense features | State-of-the-art visual understanding |\n",
    "| **SigLIP** (Google) | Improved CLIP with sigmoid loss | Better calibration, multi-modal understanding |\n",
    "\n",
    "#### Example: CLIP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEmbedding(EmbeddingModel):\n",
    "    def __init__(self, device: torch.device, model_name: str = \"ViT-B-32\"):\n",
    "        super().__init__(device)\n",
    "        self.model_name = model_name\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self) -> None:\n",
    "        import open_clip\n",
    "        \n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
    "            self.model_name, pretrained=\"openai\"\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.tokenizer = open_clip.get_tokenizer(self.model_name)\n",
    "    \n",
    "    def encode_image(self, image: Image.Image) -> torch.Tensor:\n",
    "        image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.model.encode_image(image_input)\n",
    "            features = F.normalize(features, p=2, dim=1)  # L2 normalization\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def encode_image_patches(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"Extract patch-level features from CLIP ViT.\"\"\"\n",
    "        image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vision_model = self.model.visual\n",
    "            \n",
    "            # Get patch embeddings\n",
    "            x = vision_model.conv1(image_input)\n",
    "            x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)\n",
    "            \n",
    "            # Add position embeddings and pass through transformer\n",
    "            x = torch.cat([vision_model.class_embedding + \n",
    "                          torch.zeros(...), x], dim=1)\n",
    "            x = x + vision_model.positional_embedding\n",
    "            x = vision_model.ln_pre(x)\n",
    "            \n",
    "            x = x.permute(1, 0, 2)\n",
    "            for block in vision_model.transformer.resblocks:\n",
    "                x = block(x)\n",
    "            x = x.permute(1, 0, 2)\n",
    "            \n",
    "            # Extract patch features (exclude CLS token)\n",
    "            patch_features = x[:, 1:, :]\n",
    "            patch_features = vision_model.ln_post(patch_features)\n",
    "            \n",
    "            if vision_model.proj is not None:\n",
    "                patch_features = patch_features @ vision_model.proj\n",
    "            \n",
    "            return F.normalize(patch_features, p=2, dim=-1).squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Implementation Detail**: We extract features **before the final pooling layer** to get patch-level representations. This enables the patch attention analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: Similarity Computation & Ranking\n",
    "\n",
    "Once we have candidate images, we need to rank them by visual similarity.\n",
    "\n",
    "#### Parallel Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(\n",
    "    self,\n",
    "    query_image: Image.Image,\n",
    "    candidate_urls: List[str],\n",
    "    include_patch_attention: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \n",
    "    # Encode query image once\n",
    "    query_features = self.embedding_model.encode_image(query_image)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor for concurrent downloads\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all download tasks\n",
    "        future_to_url = {\n",
    "            executor.submit(\n",
    "                self.download_and_process_image,\n",
    "                url, query_features, query_image, include_patch_attention\n",
    "            ): url\n",
    "            for url in candidate_urls\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in as_completed(future_to_url):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "                \n",
    "                # Early stopping optimization\n",
    "                target_count = 5 if include_patch_attention else 20\n",
    "                if len(results) >= target_count:\n",
    "                    # Cancel remaining futures\n",
    "                    for remaining_future in future_to_url:\n",
    "                        remaining_future.cancel()\n",
    "                    break\n",
    "    \n",
    "    # Sort by similarity score (highest first)\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    final_count = 3 if include_patch_attention else 15\n",
    "    return results[:final_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Optimizations\n",
    "\n",
    "1. **Concurrent Downloads**: Up to 10 simultaneous image downloads\n",
    "2. **Early Stopping**: Stop after getting enough good results\n",
    "3. **Future Cancellation**: Cancel remaining downloads when target is met\n",
    "4. **Global Model Reuse**: Models are loaded once and reused (singleton pattern)\n",
    "5. **GPU Acceleration**: Automatically uses GPU when available\n",
    "\n",
    "#### Similarity Computation\n",
    "\n",
    "We use **cosine similarity** between normalized feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(query_features, candidate_features):\n",
    "    # Both features are already L2-normalized\n",
    "    # Cosine similarity = dot product of normalized vectors\n",
    "    similarity = torch.mm(query_features, candidate_features.T).item()\n",
    "    return similarity  # Range: [-1, 1], typically [0.2, 0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 5: Patch-Level Attention Analysis\n",
    "\n",
    "This is one of the most interesting features. Instead of just saying \"these images are 85% similar,\" we can show **which parts of the images correspond**.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "Vision Transformers (ViT) divide images into patches. For example:\n",
    "- 224×224 image with 16×16 patches = 196 patches (14×14 grid)\n",
    "- Each patch gets its own embedding vector\n",
    "\n",
    "We compute attention between query patches and candidate patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchAttentionAnalyzer:\n",
    "    def compute_patch_similarities(\n",
    "        self,\n",
    "        query_image: Image.Image,\n",
    "        candidate_image: Image.Image\n",
    "    ) -> Dict[str, Any]:\n",
    "        \n",
    "        # Get patch features\n",
    "        query_patches = self.embedding_model.encode_image_patches(query_image)\n",
    "        # Shape: [num_query_patches, feature_dim]\n",
    "        \n",
    "        candidate_patches = self.embedding_model.encode_image_patches(candidate_image)\n",
    "        # Shape: [num_candidate_patches, feature_dim]\n",
    "        \n",
    "        # Compute attention matrix\n",
    "        attention_matrix = self.embedding_model.compute_patch_attention(\n",
    "            query_patches, candidate_patches\n",
    "        )\n",
    "        # Shape: [num_query_patches, num_candidate_patches]\n",
    "        # attention_matrix[i, j] = similarity between query patch i and candidate patch j\n",
    "        \n",
    "        # Find top correspondences\n",
    "        top_correspondences = []\n",
    "        for i in range(attention_matrix.shape[0]):\n",
    "            patch_similarities = attention_matrix[i]\n",
    "            top_indices = torch.topk(patch_similarities, k=5)\n",
    "            \n",
    "            top_correspondences.append({\n",
    "                'query_patch_idx': i,\n",
    "                'query_patch_coord': (i // grid_size, i % grid_size),\n",
    "                'top_candidate_coords': [...],\n",
    "                'similarity_scores': top_indices.values.tolist()\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'attention_matrix': attention_matrix.cpu().numpy(),\n",
    "            'top_correspondences': top_correspondences,\n",
    "            'overall_similarity': torch.mean(attention_matrix).item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "The system generates matplotlib visualizations showing:\n",
    "\n",
    "1. **Attention Heatmap**: Full matrix of patch-to-patch similarities\n",
    "2. **Top Correspondences**: Side-by-side view of matching patches\n",
    "3. **Max Attention Grid**: Heatmap showing which query patches have strongest matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_heatmap(...):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot query and candidate images with patch grids\n",
    "    axes[0, 0].imshow(query_image)\n",
    "    self._overlay_patch_grid(axes[0, 0], query_image.size, query_grid_size)\n",
    "    \n",
    "    # Plot attention matrix\n",
    "    axes[1, 0].imshow(attention_matrix, cmap='viridis')\n",
    "    \n",
    "    # Plot max attention per query patch\n",
    "    max_attention = np.max(attention_matrix, axis=1)\n",
    "    axes[1, 1].imshow(max_attention.reshape(grid_size, grid_size), cmap='hot')\n",
    "    \n",
    "    # Convert to base64 for API response\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='png', dpi=150)\n",
    "    return base64.b64encode(buffer.getvalue()).decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Native Attention Support\n",
    "\n",
    "For models like DINOv2 with registers, we can use **native attention mechanisms**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_attention(query_image, candidate_image):\n",
    "    query_patches = self.encode_image_patches(query_image)\n",
    "    candidate_patches = self.encode_image_patches(candidate_image)\n",
    "    \n",
    "    # Compute attention-style similarity\n",
    "    attention_logits = torch.mm(query_patches, candidate_patches.T)\n",
    "    \n",
    "    # Apply softmax to get attention distribution\n",
    "    cross_attention = F.softmax(attention_logits, dim=1)\n",
    "    \n",
    "    return cross_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 6: Caching & URL Validation\n",
    "\n",
    "#### Search Cache\n",
    "\n",
    "To avoid redundant searches and API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchCache:\n",
    "    def __init__(self, default_ttl: int = 3600, max_size: int = 1000):\n",
    "        self.cache: Dict[str, CacheEntry] = {}\n",
    "        self.default_ttl = default_ttl  # 1 hour\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_cache_key(query: str, max_results: int) -> str:\n",
    "        return f\"{query}_{max_results}\"\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        if key in self.cache:\n",
    "            entry = self.cache[key]\n",
    "            if time.time() < entry.expiry:\n",
    "                return entry.value\n",
    "            else:\n",
    "                del self.cache[key]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cache Strategy:**\n",
    "- LRU eviction when cache is full\n",
    "- 1-hour TTL for search results\n",
    "- Cache key includes query and max_results\n",
    "\n",
    "#### URL Validation\n",
    "\n",
    "Many scraped URLs are broken or inaccessible. We validate them concurrently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLValidator:\n",
    "    def __init__(self, max_workers: int = 10, timeout: int = 10):\n",
    "        self.max_workers = max_workers\n",
    "        self.timeout = timeout\n",
    "    \n",
    "    def validate_urls(self, urls: List[str]) -> List[str]:\n",
    "        valid_urls = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_url = {executor.submit(self._check_url, url): url \n",
    "                           for url in urls}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                if future.result():\n",
    "                    valid_urls.append(url)\n",
    "        \n",
    "        return valid_urls\n",
    "    \n",
    "    def _check_url(self, url: str) -> bool:\n",
    "        try:\n",
    "            response = requests.head(url, timeout=self.timeout, \n",
    "                                   allow_redirects=True)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive: Frontend Architecture\n",
    "\n",
    "### Technology Choices\n",
    "\n",
    "**Next.js 14 with Pages Router**\n",
    "- Server-side rendering capabilities\n",
    "- Optimized image loading\n",
    "- Easy Vercel deployment\n",
    "- Built-in API routes\n",
    "\n",
    "**TypeScript**\n",
    "- Type safety for API responses\n",
    "- Better IDE support\n",
    "- Catches bugs at compile time\n",
    "\n",
    "**Tailwind CSS**\n",
    "- Rapid UI development\n",
    "- Consistent design system\n",
    "- Small bundle size\n",
    "\n",
    "### State Management\n",
    "\n",
    "The frontend uses React Hooks for state management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// From pages/index.tsx\n",
    "\n",
    "// Search state\n",
    "const [selectedImage, setSelectedImage] = useState<File | null>(null)\n",
    "const [results, setResults] = useState<SearchResult[]>([])\n",
    "const [caption, setCaption] = useState<string>('')\n",
    "const [isLoading, setIsLoading] = useState(false)\n",
    "const [error, setError] = useState<string | null>(null)\n",
    "\n",
    "// Model configuration\n",
    "const [selectedModel, setSelectedModel] = useState<string>('clip')\n",
    "const [usedModel, setUsedModel] = useState<string>('')\n",
    "const [patchAttentionEnabled, setPatchAttentionEnabled] = useState(false)\n",
    "\n",
    "// Analysis state\n",
    "const [detailedAnalysis, setDetailedAnalysis] = useState<DetailedAttentionAnalysis | null>(null)\n",
    "const [analysisLoading, setAnalysisLoading] = useState(false)\n",
    "const [selectedResultForAnalysis, setSelectedResultForAnalysis] = useState<SearchResult | null>(null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Integration\n",
    "\n",
    "#### Search Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const handleSearch = async () => {\n",
    "  if (!selectedImage) return\n",
    "  \n",
    "  setIsLoading(true)\n",
    "  setError(null)\n",
    "  \n",
    "  // Create abort controller for timeout\n",
    "  const controller = new AbortController()\n",
    "  const timeoutId = setTimeout(() => controller.abort(), 60000) // 60s timeout\n",
    "  \n",
    "  try {\n",
    "    const formData = new FormData()\n",
    "    formData.append('file', selectedImage)\n",
    "    \n",
    "    const response = await fetch(\n",
    "      `${BACKEND_URL}/search?` +\n",
    "      `embedding_model=${selectedModel}&` +\n",
    "      `include_patch_attention=${patchAttentionEnabled}`,\n",
    "      {\n",
    "        method: 'POST',\n",
    "        body: formData,\n",
    "        signal: controller.signal\n",
    "      }\n",
    "    )\n",
    "    \n",
    "    if (!response.ok) {\n",
    "      throw new Error(`Search failed: ${response.statusText}`)\n",
    "    }\n",
    "    \n",
    "    const data: SearchResponse = await response.json()\n",
    "    \n",
    "    setResults(data.results)\n",
    "    setCaption(data.caption)\n",
    "    setUsedModel(data.embedding_model)\n",
    "    \n",
    "  } catch (error) {\n",
    "    if (error.name === 'AbortError') {\n",
    "      setError('Search timed out. Please try again.')\n",
    "    } else {\n",
    "      setError(`Search failed: ${error.message}`)\n",
    "    }\n",
    "  } finally {\n",
    "    clearTimeout(timeoutId)\n",
    "    setIsLoading(false)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed Analysis Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const handleAnalyzeAttention = async (result: SearchResult) => {\n",
    "  if (!selectedImage) return\n",
    "  \n",
    "  setAnalysisLoading(true)\n",
    "  setSelectedResultForAnalysis(result)\n",
    "  \n",
    "  try {\n",
    "    const formData = new FormData()\n",
    "    formData.append('query_file', selectedImage)\n",
    "    \n",
    "    const response = await fetch(\n",
    "      `${BACKEND_URL}/analyze-attention?` +\n",
    "      `candidate_url=${encodeURIComponent(result.url)}&` +\n",
    "      `embedding_model=${usedModel}&` +\n",
    "      `include_visualizations=true`,\n",
    "      {\n",
    "        method: 'POST',\n",
    "        body: formData\n",
    "      }\n",
    "    )\n",
    "    \n",
    "    const data: DetailedAttentionAnalysis = await response.json()\n",
    "    setDetailedAnalysis(data)\n",
    "    \n",
    "  } catch (error) {\n",
    "    setError(`Analysis failed: ${error.message}`)\n",
    "  } finally {\n",
    "    setAnalysisLoading(false)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// From types/search.ts\n",
    "\n",
    "export interface SearchResult {\n",
    "  score: number\n",
    "  url: string\n",
    "  patch_attention?: {\n",
    "    overall_similarity: number\n",
    "    query_grid_size: number\n",
    "    candidate_grid_size: number\n",
    "    attention_summary: AttentionSummary\n",
    "  }\n",
    "}\n",
    "\n",
    "export interface SearchResponse {\n",
    "  caption: string\n",
    "  results: SearchResult[]\n",
    "  embedding_model: string\n",
    "  patch_attention_enabled?: boolean\n",
    "}\n",
    "\n",
    "export interface DetailedAttentionAnalysis {\n",
    "  query_image_size: [number, number]\n",
    "  candidate_image_size: [number, number]\n",
    "  candidate_url: string\n",
    "  embedding_model: string\n",
    "  similarity_analysis: AttentionSummary\n",
    "  attention_matrix_shape: [number, number]\n",
    "  top_correspondences: PatchCorrespondence[]\n",
    "  visualizations?: {\n",
    "    attention_heatmap: string  // base64 data URL\n",
    "    top_correspondences: string  // base64 data URL\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components\n",
    "\n",
    "#### ImageUpload Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export default function ImageUpload({ onImageSelect }) {\n",
    "  const [dragActive, setDragActive] = useState(false)\n",
    "  const [preview, setPreview] = useState<string | null>(null)\n",
    "  \n",
    "  const handleDrop = (e: React.DragEvent) => {\n",
    "    e.preventDefault()\n",
    "    setDragActive(false)\n",
    "    \n",
    "    const file = e.dataTransfer.files[0]\n",
    "    if (file && file.type.startsWith('image/')) {\n",
    "      onImageSelect(file)\n",
    "      setPreview(URL.createObjectURL(file))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return (\n",
    "    <div\n",
    "      onDrop={handleDrop}\n",
    "      onDragOver={(e) => { e.preventDefault(); setDragActive(true) }}\n",
    "      className={`border-2 border-dashed rounded-lg p-8 ${\n",
    "        dragActive ? 'border-blue-500 bg-blue-50' : 'border-gray-300'\n",
    "      }`}\n",
    "    >\n",
    "      {preview ? (\n",
    "        <img src={preview} alt=\"Preview\" className=\"max-h-64 mx-auto\" />\n",
    "      ) : (\n",
    "        <div className=\"text-center\">\n",
    "          <p>Drag and drop an image or click to upload</p>\n",
    "        </div>\n",
    "      )}\n",
    "    </div>\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SearchResults Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export default function SearchResults({ results, onAnalyze }) {\n",
    "  return (\n",
    "    <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6\">\n",
    "      {results.map((result, idx) => (\n",
    "        <div key={idx} className=\"border rounded-lg p-4 shadow-md\">\n",
    "          <RobustImage \n",
    "            src={result.url} \n",
    "            alt={`Result ${idx + 1}`}\n",
    "            className=\"w-full h-64 object-cover rounded\"\n",
    "          />\n",
    "          \n",
    "          <div className=\"mt-4\">\n",
    "            <p className=\"font-semibold\">\n",
    "              Similarity: {(result.score * 100).toFixed(1)}%\n",
    "            </p>\n",
    "            \n",
    "            {result.patch_attention && (\n",
    "              <p className=\"text-sm text-gray-600\">\n",
    "                Patch Attention: {result.patch_attention.overall_similarity.toFixed(2)}\n",
    "              </p>\n",
    "            )}\n",
    "            \n",
    "            <button\n",
    "              onClick={() => onAnalyze(result)}\n",
    "              className=\"mt-2 px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600\"\n",
    "            >\n",
    "              Analyze\n",
    "            </button>\n",
    "          </div>\n",
    "        </div>\n",
    "      ))}\n",
    "    </div>\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RobustImage Component\n",
    "\n",
    "Handles broken image URLs gracefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export default function RobustImage({ src, alt, className }) {\n",
    "  const [error, setError] = useState(false)\n",
    "  \n",
    "  if (error) {\n",
    "    return (\n",
    "      <div className={`${className} bg-gray-200 flex items-center justify-center`}>\n",
    "        <span className=\"text-gray-500\">Image unavailable</span>\n",
    "      </div>\n",
    "    )\n",
    "  }\n",
    "  \n",
    "  return (\n",
    "    <Image\n",
    "      src={src}\n",
    "      alt={alt}\n",
    "      className={className}\n",
    "      onError={() => setError(true)}\n",
    "    />\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "### Backend Deployment (HuggingFace Spaces)\n",
    "\n",
    "#### Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    git \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements and install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Set environment variables\n",
    "ENV PORT=7860\n",
    "ENV TRANSFORMERS_CACHE=/app/cache\n",
    "ENV HF_HOME=/app/cache\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 7860\n",
    "\n",
    "# Run the application\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Required\n",
    "HF_TOKEN=your_huggingface_token_here\n",
    "\n",
    "# Optional\n",
    "PORT=7860\n",
    "TRANSFORMERS_CACHE=/app/cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Requirements\n",
    "\n",
    "- **Recommended**: T4 Small (15GB VRAM) or higher\n",
    "- **Minimum**: CPU-only (slower, not recommended for production)\n",
    "- Models auto-download and cache on first run\n",
    "\n",
    "### Frontend Deployment (Vercel)\n",
    "\n",
    "#### vercel.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vercel.json\n",
    "{\n",
    "  \"env\": {\n",
    "    \"NEXT_PUBLIC_BACKEND_URL\": \"https://onurcopur-tattoo-search-engine.hf.space\"\n",
    "  },\n",
    "  \"build\": {\n",
    "    \"env\": {\n",
    "      \"NEXT_PUBLIC_BACKEND_URL\": \"https://onurcopur-tattoo-search-engine.hf.space\"\n",
    "    }\n",
    "  },\n",
    "  \"functions\": {\n",
    "    \"pages/api/**/*.ts\": {\n",
    "      \"maxDuration\": 30\n",
    "    }\n",
    "  },\n",
    "  \"headers\": [\n",
    "    {\n",
    "      \"source\": \"/(.*)\",\n",
    "      \"headers\": [\n",
    "        {\n",
    "          \"key\": \"X-Content-Type-Options\",\n",
    "          \"value\": \"nosniff\"\n",
    "        },\n",
    "        {\n",
    "          \"key\": \"X-Frame-Options\",\n",
    "          \"value\": \"DENY\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install Vercel CLI\n",
    "npm install -g vercel\n",
    "\n",
    "# Deploy\n",
    "cd tattoo_search_engine_frontend\n",
    "vercel --prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarks\n",
    "\n",
    "Here are some performance metrics from production:\n",
    "\n",
    "### Backend Performance\n",
    "\n",
    "| Operation | Time (GPU) | Time (CPU) |\n",
    "|-----------|------------|------------|\n",
    "| Caption Generation (GLM-4.5V) | ~2-3s | ~5-8s |\n",
    "| Multi-Platform Search | ~3-5s | ~3-5s |\n",
    "| URL Validation (100 URLs) | ~2-3s | ~2-3s |\n",
    "| CLIP Encoding (single image) | ~50ms | ~200ms |\n",
    "| DINOv2 Encoding (single image) | ~80ms | ~350ms |\n",
    "| Similarity Computation (50 images) | ~5-7s | ~15-20s |\n",
    "| Patch Attention Analysis | ~1-2s | ~4-6s |\n",
    "| **Total Search (without attention)** | **~12-18s** | **~25-35s** |\n",
    "| **Total Search (with attention)** | **~15-23s** | **~35-50s** |\n",
    "\n",
    "### Memory Usage\n",
    "\n",
    "| Model | VRAM (GPU) | RAM (CPU) |\n",
    "|-------|------------|----------|\n",
    "| CLIP ViT-B/32 | ~1.5GB | ~2GB |\n",
    "| DINOv2 ViT-B/14 | ~1.8GB | ~2.5GB |\n",
    "| DINOv3 ViT-S/16 | ~1.2GB | ~1.8GB |\n",
    "| SigLIP Base | ~1.6GB | ~2.2GB |\n",
    "\n",
    "### Cache Hit Rates\n",
    "\n",
    "- Search Cache: ~35-40% hit rate in production\n",
    "- Average time saved per cache hit: ~3-5 seconds\n",
    "\n",
    "### Optimization Impact\n",
    "\n",
    "| Optimization | Speed Improvement |\n",
    "|--------------|------------------|\n",
    "| Early stopping | 40-60% faster |\n",
    "| Concurrent downloads | 3-5x faster |\n",
    "| Search caching | 30-40% faster (on hits) |\n",
    "| GPU acceleration | 3-4x faster |\n",
    "| URL validation | Filters ~20-30% bad URLs |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Technical Challenges & Solutions\n",
    "\n",
    "### Challenge 1: Web Scraping Reliability\n",
    "\n",
    "**Problem**: Many image URLs from web scraping are broken or blocked.\n",
    "\n",
    "**Solution**:\n",
    "1. Concurrent URL validation before downloading\n",
    "2. Platform-specific headers (Pinterest, Instagram)\n",
    "3. Retry logic with exponential backoff\n",
    "4. Multiple platform fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform-specific headers\n",
    "if \"pinterest\" in url.lower():\n",
    "    headers.update({\n",
    "        \"Referer\": \"https://www.pinterest.com/\",\n",
    "        \"X-Pinterest-Source\": \"web\",\n",
    "    })\n",
    "\n",
    "# Retry with exponential backoff\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        return process_image(response)\n",
    "    except Exception as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            wait_time = (2**attempt) + random.uniform(0, 1)\n",
    "            time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Model Loading Time\n",
    "\n",
    "**Problem**: Loading embedding models takes 5-10 seconds.\n",
    "\n",
    "**Solution**: Global singleton pattern with lazy initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable\n",
    "search_engine = None\n",
    "\n",
    "def get_search_engine(embedding_model: str) -> TattooSearchEngine:\n",
    "    global search_engine\n",
    "    \n",
    "    # Reuse if same model\n",
    "    if (search_engine is None or \n",
    "        search_engine.embedding_model.get_model_name() != embedding_model):\n",
    "        search_engine = TattooSearchEngine(embedding_model)\n",
    "    \n",
    "    return search_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Patch Extraction from ViT Models\n",
    "\n",
    "**Problem**: Vision Transformers don't expose patch features by default.\n",
    "\n",
    "**Solution**: Hook into intermediate layers before pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CLIP\n",
    "x = vision_model.conv1(image_input)  # Patch embeddings\n",
    "x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)\n",
    "\n",
    "# Add positional embeddings\n",
    "x = torch.cat([class_embedding, x], dim=1)\n",
    "x = x + positional_embedding\n",
    "\n",
    "# Pass through transformer\n",
    "for block in transformer_blocks:\n",
    "    x = block(x)\n",
    "\n",
    "# Extract patches (exclude CLS token)\n",
    "patch_features = x[:, 1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Frontend Timeout Issues\n",
    "\n",
    "**Problem**: Some searches take >30 seconds, causing timeouts.\n",
    "\n",
    "**Solution**: \n",
    "1. 60-second timeout with AbortController\n",
    "2. Loading indicators with progress updates\n",
    "3. Early stopping in backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const controller = new AbortController()\n",
    "const timeoutId = setTimeout(() => controller.abort(), 60000)\n",
    "\n",
    "try {\n",
    "  const response = await fetch(url, {\n",
    "    signal: controller.signal,\n",
    "    // ...\n",
    "  })\n",
    "} catch (error) {\n",
    "  if (error.name === 'AbortError') {\n",
    "    setError('Request timed out. Please try again.')\n",
    "  }\n",
    "} finally {\n",
    "  clearTimeout(timeoutId)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: Cross-Origin Image Loading\n",
    "\n",
    "**Problem**: Many tattoo images are hosted on domains with CORS restrictions.\n",
    "\n",
    "**Solution**: \n",
    "1. Backend downloads and serves images\n",
    "2. Frontend uses RobustImage component with fallback\n",
    "3. Next.js Image component with wildcard domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// next.config.js\n",
    "module.exports = {\n",
    "  images: {\n",
    "    remotePatterns: [\n",
    "      { protocol: 'https', hostname: '**' },\n",
    "      { protocol: 'http', hostname: '**' }\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "\n",
    "### Short-term\n",
    "\n",
    "1. **Vector Database Integration**\n",
    "   - Pre-compute embeddings for popular tattoo images\n",
    "   - Use Pinecone/Weaviate for fast similarity search\n",
    "   - Reduce search time from 15s to <2s\n",
    "\n",
    "2. **Better Scraping**\n",
    "   - Direct Pinterest API integration\n",
    "   - Instagram Graph API\n",
    "   - Dedicated tattoo databases (Tattoodo, InkHunter)\n",
    "\n",
    "3. **Advanced Filtering**\n",
    "   - Filter by style (traditional, realistic, watercolor)\n",
    "   - Filter by body placement\n",
    "   - Filter by color scheme\n",
    "\n",
    "4. **User Feedback Loop**\n",
    "   - Allow users to mark results as relevant/irrelevant\n",
    "   - Use feedback to fine-tune ranking\n",
    "   - Personalized recommendations\n",
    "\n",
    "### Long-term\n",
    "\n",
    "1. **Custom Fine-tuned Models**\n",
    "   - Fine-tune CLIP/DINOv2 on tattoo-specific dataset\n",
    "   - Better understanding of tattoo styles and elements\n",
    "\n",
    "2. **Generative Features**\n",
    "   - \"Similar but different\" - generate variations\n",
    "   - Style transfer between tattoos\n",
    "   - Text-to-tattoo generation\n",
    "\n",
    "3. **Mobile App**\n",
    "   - Native iOS/Android apps\n",
    "   - Camera integration for on-the-go search\n",
    "   - Offline mode with cached embeddings\n",
    "\n",
    "4. **Artist Marketplace Integration**\n",
    "   - Connect users with tattoo artists\n",
    "   - Portfolio matching based on style\n",
    "   - Booking system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Development Setup\n",
    "\n",
    "Want to run this project locally? Here's how:\n",
    "\n",
    "### Backend Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone the repository\n",
    "git clone <your-repo-url>\n",
    "cd tattoo_project/tattoo_search_engine\n",
    "\n",
    "# Create virtual environment\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Create .env file\n",
    "echo \"HF_TOKEN=your_huggingface_token\" > .env\n",
    "\n",
    "# Run the server\n",
    "python app.py\n",
    "# Server will start on http://localhost:7860"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frontend Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# In a new terminal\n",
    "cd tattoo_project/tattoo_search_engine_frontend\n",
    "\n",
    "# Install dependencies\n",
    "npm install\n",
    "\n",
    "# Create .env.local\n",
    "echo \"NEXT_PUBLIC_BACKEND_URL=http://localhost:7860\" > .env.local\n",
    "\n",
    "# Run development server\n",
    "npm run dev\n",
    "# Frontend will start on http://localhost:3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test backend health\n",
    "curl http://localhost:7860/health\n",
    "\n",
    "# Test available models\n",
    "curl http://localhost:7860/models\n",
    "\n",
    "# Test search endpoint\n",
    "curl -X POST http://localhost:7860/search \\\n",
    "  -F \"file=@test_tattoo.jpg\" \\\n",
    "  -F \"embedding_model=clip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Building this tattoo search engine was an exciting journey through modern AI and web technologies. Here are the key takeaways:\n",
    "\n",
    "### Technical Highlights\n",
    "\n",
    "1. **Multi-Model Architecture**: Supporting multiple embedding models (CLIP, DINOv2, SigLIP) provides flexibility and lets users choose the best model for their use case.\n",
    "\n",
    "2. **Patch-Level Analysis**: Going beyond global similarity to understand which parts of images correspond provides valuable insights and improves result quality.\n",
    "\n",
    "3. **Production-Ready Design**: Caching, concurrent processing, error handling, and GPU optimization make this suitable for real-world use.\n",
    "\n",
    "4. **Full-Stack Integration**: Seamless connection between Python ML backend and TypeScript Next.js frontend demonstrates modern web development practices.\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "- **Web scraping is hard**: URL validation and platform-specific handling are crucial\n",
    "- **Performance matters**: Early stopping and concurrent processing provide 3-5x speedups\n",
    "- **User experience is key**: Clear loading states, error messages, and fallbacks make the difference\n",
    "- **Type safety helps**: TypeScript caught many API integration bugs before production\n",
    "\n",
    "### Try It Yourself\n",
    "\n",
    "Visit the live demo at [https://tattoo-search-frontend.vercel.app](https://tattoo-search-frontend.vercel.app) or follow the setup instructions above to run it locally.\n",
    "\n",
    "The code demonstrates practical applications of:\n",
    "- Vision transformers (CLIP, DINOv2, SigLIP)\n",
    "- Image similarity search\n",
    "- Attention mechanism visualization\n",
    "- FastAPI backend development\n",
    "- Next.js frontend development\n",
    "- Docker deployment\n",
    "- Production ML system design\n",
    "\n",
    "I hope this deep dive was helpful! Feel free to reach out with questions or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Specifications Summary\n",
    "\n",
    "### Backend Stack\n",
    "```yaml\n",
    "Framework: FastAPI 0.100+\n",
    "Python: 3.9+\n",
    "ML Libraries:\n",
    "  - PyTorch 2.0+\n",
    "  - Transformers 4.30+\n",
    "  - OpenCLIP 2.20+\n",
    "  - TIMM 0.9+\n",
    "Vision Models:\n",
    "  - CLIP (OpenAI ViT-B/32)\n",
    "  - DINOv2 (Meta ViT-B/14)\n",
    "  - DINOv2 with Registers\n",
    "  - DINOv3 (Meta ViT-S/16)\n",
    "  - SigLIP (Google Base)\n",
    "Captioning: GLM-4.5V via HuggingFace InferenceClient\n",
    "Web Scraping: DuckDuckGo Search, Requests, LXML\n",
    "Deployment: Docker on HuggingFace Spaces\n",
    "GPU: T4 Small (15GB VRAM recommended)\n",
    "```\n",
    "\n",
    "### Frontend Stack\n",
    "```yaml\n",
    "Framework: Next.js 14 (Pages Router)\n",
    "Language: TypeScript 5.3+\n",
    "Styling: Tailwind CSS 3.4+\n",
    "Node: 18.0+\n",
    "Deployment: Vercel\n",
    "```\n",
    "\n",
    "### API Endpoints\n",
    "```\n",
    "POST /search\n",
    "  Parameters:\n",
    "    - file: Image file (multipart/form-data)\n",
    "    - embedding_model: clip|dinov2|dinov3|siglip (default: clip)\n",
    "    - include_patch_attention: boolean (default: false)\n",
    "  Returns: SearchResponse with results, caption, model info\n",
    "\n",
    "POST /analyze-attention\n",
    "  Parameters:\n",
    "    - query_file: Query image file\n",
    "    - candidate_url: URL of candidate image\n",
    "    - embedding_model: Model to use\n",
    "    - include_visualizations: boolean (default: true)\n",
    "  Returns: DetailedAttentionAnalysis with visualizations\n",
    "\n",
    "GET /models\n",
    "  Returns: Available models and their configurations\n",
    "\n",
    "GET /health\n",
    "  Returns: Health check status\n",
    "```\n",
    "\n",
    "### Performance Metrics\n",
    "```\n",
    "Search Latency (GPU):\n",
    "  - Without patch attention: 12-18s\n",
    "  - With patch attention: 15-23s\n",
    "\n",
    "Memory Requirements:\n",
    "  - VRAM (GPU): 1.5-2GB per model\n",
    "  - RAM: 4-6GB total\n",
    "\n",
    "Cache:\n",
    "  - TTL: 1 hour\n",
    "  - Max Size: 1000 entries\n",
    "  - Hit Rate: 35-40%\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
